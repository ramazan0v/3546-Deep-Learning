{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TLDR: BERT Encoder for Dialogue Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "443WoH9rgZwm"
      },
      "source": [
        "#**TLDR** (Transfer Learning: Deep Recap) by *Ramazan Ramazanov*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa-JIU9OLRsx"
      },
      "source": [
        "TLDR: BERT Encoder for Dialogue Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgleEI4-gNs0"
      },
      "source": [
        "##Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3jTppDVgNPO"
      },
      "source": [
        "All customer service calls to the businesses are recorded for \"quality and training\" purposes. Nowadays, we have mostly reliable speech-to-text technologies. Based on my professional background, I understand how important it is to monitor these calls to ensure good quality customer service, proper training and compliance. Most of these calls are lengthy, filled with template greetings and irrelevant chit-chat. This, coupled with the thousands of calls per day, makes the quality checks <1% of total call volume. That's why I tried to tackle the problem of dialogue summarization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2unAk9Z6aY7j"
      },
      "source": [
        "We briefly covered Transformer model architecture but never explored it in detail. I apply the language understanding model from the Tensorflow tutorial **[1]** to train it for dialogue summarization. Then I replace the encoder with the BERT model while keeping the decoder part the same. BERT has state-of-the-art performance when applied to various NLP projects, and I wanted to use its power to summarize dialogues. Finally, a pre-trained T5 transformer was used to compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnGwCSACPpmN"
      },
      "source": [
        "The dataset I'm using is the SAMSum corpus **[2]**. It is \"A Human-annotated Dialogue Dataset for Abstractive Summarization\". Abstractive summarization is an interpretative technique, and it can be paraphrase instead of quoting the text directly. It's different than extractive summarization, which highlights and quotes essential parts of the text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOb1yNwMSJRM"
      },
      "source": [
        "For both models, tokenizers from hugging face were used **[3]**. BERT model is also called using the *transformers* package. As will be mentioned below, a small BERT model was used to avoid out-of-memory issues and long runtimes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WQ_WIIWgR_r"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMYazpqVg_ac"
      },
      "source": [
        "We start by installing tensorflow_text package and transformers from huggingface:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZzAneHfKNxA"
      },
      "source": [
        "pip install -q tensorflow_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_09aPrJLdvFN"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGpZtUiyakcM"
      },
      "source": [
        "Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSl1nO4yJzLf"
      },
      "source": [
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lN7WpzlQt7p",
        "outputId": "69a1cc2a-bfe3-4136-fd35-97e29fd6b20b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LITv_pcVaqRk"
      },
      "source": [
        "Open SAMSum dataset. It has Testing, Training and Validation datasets in json files. To save memory, we only extract training and testing datasets into pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek1twXZ0QwQ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "28824888-6557-48c7-8ac0-788808fb3d56"
      },
      "source": [
        "data_dir = '/content/gdrive/My Drive/3546 Project/SAMSum/'\n",
        "\n",
        "with open(data_dir + 'train.json', encoding=\"utf8\") as f:\n",
        "    train = json.load(f)\n",
        "train_df = pd.DataFrame(train)\n",
        "del train\n",
        "\n",
        "with open(data_dir + 'test.json', encoding=\"utf8\") as f:\n",
        "    test = json.load(f)\n",
        "test_df = pd.DataFrame(test)\n",
        "del test\n",
        "\n",
        "'''\n",
        "with open(data_dir + 'val.json', encoding=\"utf8\") as f:\n",
        "    val = json.load(f)\n",
        "val_df = pd.DataFrame(val)\n",
        "del val\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nwith open(data_dir + \\'val.json\\', encoding=\"utf8\") as f:\\n    val = json.load(f)\\nval_df = pd.DataFrame(val)\\ndel val\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mEN8IqzbJ3V"
      },
      "source": [
        "Some pre-processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw9CBSD2Qz4G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "70ae64c7-d158-4e0b-c209-aa39826c896b"
      },
      "source": [
        "#Drop id column from the dataframe, and remove end-of-line characters\n",
        "train_df.drop(columns='id', inplace=True)\n",
        "train_df['summary'] = train_df['summary'].str.replace('\\r', ' ')\n",
        "train_df['summary'] = train_df['summary'].str.replace('\\n', ' ')\n",
        "train_df['dialogue'] = train_df['dialogue'].str.replace('\\r', ' ')\n",
        "train_df['dialogue'] = train_df['dialogue'].str.replace('\\n', ' ')\n",
        "\n",
        "test_df.drop(columns='id', inplace=True)\n",
        "test_df['summary'] = test_df['summary'].str.replace('\\r', ' ')\n",
        "test_df['summary'] = test_df['summary'].str.replace('\\n', ' ')\n",
        "test_df['dialogue'] = test_df['dialogue'].str.replace('\\r', ' ')\n",
        "test_df['dialogue'] = test_df['dialogue'].str.replace('\\n', ' ')\n",
        "\n",
        "'''\n",
        "val_df['summary'] = val_df['summary'].str.replace('\\r', ' ')\n",
        "val_df['summary'] = val_df['summary'].str.replace('\\n', ' ')\n",
        "val_df['dialogue'] = val_df['dialogue'].str.replace('\\r', ' ')\n",
        "val_df['dialogue'] = val_df['dialogue'].str.replace('\\n', ' ')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nval_df['summary'] = val_df['summary'].str.replace('\\r', ' ')\\nval_df['summary'] = val_df['summary'].str.replace('\\n', ' ')\\nval_df['dialogue'] = val_df['dialogue'].str.replace('\\r', ' ')\\nval_df['dialogue'] = val_df['dialogue'].str.replace('\\n', ' ')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gef1JIN4bOk1"
      },
      "source": [
        "We can utilize pre-trained BERT tokenizer from huggingface:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8DB0n6oPYtd"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9RWDIo9bXSs"
      },
      "source": [
        "Define a function which will create tokenized dialogues and their corresponding summaries as tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja6PXmtRRCLn"
      },
      "source": [
        "def tokenize_pairs(dialg, summ):\n",
        "    dialg = tf.dtypes.cast(tokenizer(dialg, padding=True, truncation=True, return_tensors=\"tf\")['input_ids'], tf.int64)\n",
        "    summ = tf.dtypes.cast(tokenizer(summ, padding=True, truncation=True, return_tensors=\"tf\")['input_ids'], tf.int64)\n",
        "    return dialg, summ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIdFIjqocR-E"
      },
      "source": [
        "Getting the final tokenized dataset ready:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHnGnVZmYps-"
      },
      "source": [
        "#Divide the tokenized tensors into batches of size 50\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "dialg, summ = tokenize_pairs(train_df.iloc[:, 1].to_list(), train_df.iloc[:, 0].to_list())\n",
        "\n",
        "#Combine tokens and make them into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((dialg, summ))\n",
        "train_ds = train_ds.batch(BATCH_SIZE)\n",
        "\n",
        "#Delete dataframes to save memory\n",
        "del train_df\n",
        "del dialg\n",
        "del summ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mtXrJ6vcYSG"
      },
      "source": [
        "##Building the Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b37A41CIaM2L"
      },
      "source": [
        "As mentioned previously, I modified the Transformer model from Tensorflow tutorials **[1]**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa9WynQweO2r"
      },
      "source": [
        "For positional encoding, we build our custom functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6VvURnhKSd-"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ochcj-MQK3L1"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBIZCHHAenJ1"
      },
      "source": [
        "Creating functions to mask padding and hide future tokens in a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSnMfkSDK8cG"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCyMmGcULG_2"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0ZL70gAbWVa"
      },
      "source": [
        "For q (query), k (key) and v (value), create dot product attention:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvz_Iw_lLJSo"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPfGBQpSfp2s"
      },
      "source": [
        "By generalizing the above dot product attention, create multi head attention for q,k,v:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkhx0geBLMg2"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE5pG3T4gRbm"
      },
      "source": [
        "Both encoder and decoder have feed forward networks post multi-head attention. For FFN, we have (dff to d_model) fully connected hidden layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxXF-kjQLQKN"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXyZTm0hhfB9"
      },
      "source": [
        "Define Encoder with multi-head attention and FFN defined above, and add normalization and dropout for optimization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gmVHHleLWPt"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6dm2BNHiCgw"
      },
      "source": [
        "Decoder applies masked multi-head attention to the output embeddings, another multi-head attention to the encoder outputs and adds a FFN to generate output probabilities using softmax. That's why we have 3 normalization and dropout layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBEJT2BKLW71"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBGm2zedim9M"
      },
      "source": [
        "Build full Encoder and Decoder. Note that we will use Encoder only for the first model, later BERT encoder will replace this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S8QDipNLbZk"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoOP9uw7LdLc"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU5GVvGshuPJ"
      },
      "source": [
        "This is where we utilize Transfer Learning. Define two Transformers, one with the custom built encoder we defined above, the other with the BERT model as encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ1uVd5eLeya"
      },
      "source": [
        "class TransformerCustom(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(TransformerCustom, self).__init__()\n",
        "\n",
        "    #Call custom built Encoder\n",
        "    self.tokenizer = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inp, tar, training, enc_padding_mask,\n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqzzULEUp1z8"
      },
      "source": [
        "class TransformerBERT(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(TransformerBERT, self).__init__()\n",
        "\n",
        "    #Call Bert encoder\n",
        "    self.tokenizer = TFBertModel(BertConfig(hidden_size=d_model,\n",
        "                                            num_hidden_layers=num_layers,\n",
        "                                            num_attention_heads=num_heads))\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inp, tar, training, enc_padding_mask,\n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    #We need the sequence of hidden-states at the output of the last layer of the BERT model\n",
        "    enc_output = self.tokenizer(inp).last_hidden_state # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVVRWo89jbV4"
      },
      "source": [
        "Due to the lack of resources, the model cannot be as complex. I realized the default parameters worked fine, and increasing them led to out-of-memory issues. For instance, the model could not handle bert-base model, which has 12 layers, 768 hidden dimensions, 12 heads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpj7WqpyLgro"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHcjuHpxrbPP"
      },
      "source": [
        "Create Adam optimizer with a custom learning schedule:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnJY1N4QLigd"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39b_FsY1kPe8"
      },
      "source": [
        "Adam optimizer has adaptable learning rate, so it is auto-tuning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5pFHfXXVZXs"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-7OV3ERLjF2"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW9BHPVtr7c_"
      },
      "source": [
        "Defining loss and accuracy metrics. We apply masking because we have padded embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZC3qzDtLlH4"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ujfSdibn1a"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI8xJmuIkg2P"
      },
      "source": [
        "Initialize the transformer classes with the above parameters. According to BERT documentation, the English vocabulary size for the model is 30,522:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y4Li7WaLpqp"
      },
      "source": [
        "transformerCustom = TransformerCustom(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size= tf.convert_to_tensor(30522, dtype=tf.int32),\n",
        "    target_vocab_size= tf.convert_to_tensor(30522, dtype=tf.int32),\n",
        "    pe_input=600, #this is max positional encoding\n",
        "    pe_target=600, #this is max positional encoding\n",
        "    rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b95ggC3ysKPr"
      },
      "source": [
        "transformerBERT = TransformerBERT(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size= tf.convert_to_tensor(30522, dtype=tf.int32),\n",
        "    target_vocab_size= tf.convert_to_tensor(30522, dtype=tf.int32),\n",
        "    pe_input=600, #this is max positional encoding\n",
        "    pe_target=600, #this is max positional encoding\n",
        "    rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ8FF8iHsh0q"
      },
      "source": [
        "Create encoder, decoder and look ahead masks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbV4lqKgLsjs"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by\n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIfopsx2tgF1"
      },
      "source": [
        "Create and keep checkpoints for both models every 5 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSkTdl8fLvSS"
      },
      "source": [
        "checkpoint_path_cust = \"./checkpoints/cust/train\"\n",
        "checkpoint_path_bert = \"./checkpoints/bert/train\"\n",
        "\n",
        "ckptCustom = tf.train.Checkpoint(transformer=transformerCustom,\n",
        "                           optimizer=optimizer)\n",
        "ckptBERT = tf.train.Checkpoint(transformer=transformerBERT,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager_cust = tf.train.CheckpointManager(ckptCustom, \n",
        "                                               checkpoint_path_cust, \n",
        "                                               max_to_keep=5)\n",
        "\n",
        "ckpt_manager_bert = tf.train.CheckpointManager(ckptBERT, \n",
        "                                               checkpoint_path_bert, \n",
        "                                               max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager_cust.latest_checkpoint:\n",
        "  ckptCustom.restore(ckpt_manager_cust.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')\n",
        "\n",
        "if ckpt_manager_bert.latest_checkpoint:\n",
        "  ckptBERT.restore(ckpt_manager_bert.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcB2nRAiWGoT"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUzTaVOPYbDI"
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64)\n",
        "]\n",
        "\n",
        "\n",
        "#Create a function to train with the custom encoder\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step_custom(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  #GradientTape uses Custom Encoder\n",
        "  with tf.GradientTape() as tape:\n",
        "      predictions, _ = transformerCustom(inp, tar_inp,\n",
        "                                  True,\n",
        "                                  enc_padding_mask,\n",
        "                                  combined_mask,\n",
        "                                  dec_padding_mask)\n",
        "      loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformerCustom.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformerCustom.trainable_variables))\n",
        "      \n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))\n",
        "\n",
        "\n",
        "#Create a function to train with the BERT encoder\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step_bert(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  #Call Transformer with BERT encoder\n",
        "  with tf.GradientTape() as tape:\n",
        "      predictions, _ = transformerBERT(inp, tar_inp,\n",
        "                                  True,\n",
        "                                  enc_padding_mask,\n",
        "                                  combined_mask,\n",
        "                                  dec_padding_mask)\n",
        "      loss = loss_function(tar_real, predictions)\n",
        "      \n",
        "  gradients = tape.gradient(loss, transformerBERT.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformerBERT.trainable_variables))\n",
        "      \n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcDIYsqp1eYj"
      },
      "source": [
        "After we train the transformer, we define the following function to generate summaries for the input sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVDaJie81dkX"
      },
      "source": [
        "def evaluate(sentence, max_length=50, use_bert=False):\n",
        "  # inp sentence is the dialogue, hence adding the start and end token\n",
        "\n",
        "  sentence = tf.dtypes.cast(tokenizer(sentence, padding=True, truncation=True, return_tensors=\"tf\")['input_ids'], tf.int64)\n",
        "\n",
        "  encoder_input = sentence\n",
        "\n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  start, end = tokenizer([''])['input_ids'][0]\n",
        "  output = tf.convert_to_tensor([start])\n",
        "  output = tf.dtypes.cast(tf.expand_dims(output, 0), tf.int64)\n",
        "\n",
        "  for i in range(max_length):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    if use_bert:\n",
        "        predictions, attention_weights = transformerBERT(encoder_input,\n",
        "                                                    output,\n",
        "                                                    False,\n",
        "                                                    enc_padding_mask,\n",
        "                                                    combined_mask,\n",
        "                                                    dec_padding_mask)\n",
        "    else:\n",
        "        predictions, attention_weights = transformerCustom(encoder_input,\n",
        "                                                    output,\n",
        "                                                    False,\n",
        "                                                    enc_padding_mask,\n",
        "                                                    combined_mask,\n",
        "                                                    dec_padding_mask)\n",
        "\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == end:\n",
        "      break\n",
        "\n",
        "  text = tokenizer.decode(output[0])\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRWZZzPc2CFq"
      },
      "source": [
        "##Custom Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZClb3XptrSf"
      },
      "source": [
        "Compile and fit the transformer with the custom encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8QAj1YIPCZo",
        "outputId": "cc7e3307-8af1-47fd-9976-c56976767354"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> dialogue, tar -> summary\n",
        "  for (batch, (inp, tar)) in enumerate(train_ds):\n",
        "    train_step_custom(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager_cust.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 10.3286 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 10.2825 Accuracy 0.0258\n",
            "Epoch 1 Batch 100 Loss 10.1952 Accuracy 0.0512\n",
            "Epoch 1 Batch 150 Loss 10.0671 Accuracy 0.0598\n",
            "Epoch 1 Batch 200 Loss 9.8963 Accuracy 0.0642\n",
            "Epoch 1 Batch 250 Loss 9.6873 Accuracy 0.0670\n",
            "Epoch 1 Loss 9.4806 Accuracy 0.0685\n",
            "Time taken for 1 epoch: 71.99 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 8.0209 Accuracy 0.0773\n",
            "Epoch 2 Batch 50 Loss 7.7415 Accuracy 0.0773\n",
            "Epoch 2 Batch 100 Loss 7.4768 Accuracy 0.0772\n",
            "Epoch 2 Batch 150 Loss 7.2567 Accuracy 0.0772\n",
            "Epoch 2 Batch 200 Loss 7.0935 Accuracy 0.0814\n",
            "Epoch 2 Batch 250 Loss 6.9667 Accuracy 0.0882\n",
            "Epoch 2 Loss 6.8728 Accuracy 0.0929\n",
            "Time taken for 1 epoch: 59.13 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 6.2913 Accuracy 0.1255\n",
            "Epoch 3 Batch 50 Loss 6.1904 Accuracy 0.1446\n",
            "Epoch 3 Batch 100 Loss 6.0847 Accuracy 0.1566\n",
            "Epoch 3 Batch 150 Loss 5.9855 Accuracy 0.1639\n",
            "Epoch 3 Batch 200 Loss 5.8959 Accuracy 0.1703\n",
            "Epoch 3 Batch 250 Loss 5.8114 Accuracy 0.1761\n",
            "Epoch 3 Loss 5.7488 Accuracy 0.1801\n",
            "Time taken for 1 epoch: 59.25 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 5.3937 Accuracy 0.1860\n",
            "Epoch 4 Batch 50 Loss 5.3262 Accuracy 0.2063\n",
            "Epoch 4 Batch 100 Loss 5.2796 Accuracy 0.2082\n",
            "Epoch 4 Batch 150 Loss 5.2391 Accuracy 0.2117\n",
            "Epoch 4 Batch 200 Loss 5.2057 Accuracy 0.2140\n",
            "Epoch 4 Batch 250 Loss 5.1720 Accuracy 0.2163\n",
            "Epoch 4 Loss 5.1479 Accuracy 0.2178\n",
            "Time taken for 1 epoch: 59.19 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 5.0182 Accuracy 0.2152\n",
            "Epoch 5 Batch 50 Loss 4.9916 Accuracy 0.2273\n",
            "Epoch 5 Batch 100 Loss 4.9588 Accuracy 0.2290\n",
            "Epoch 5 Batch 150 Loss 4.9330 Accuracy 0.2311\n",
            "Epoch 5 Batch 200 Loss 4.9117 Accuracy 0.2329\n",
            "Epoch 5 Batch 250 Loss 4.8884 Accuracy 0.2346\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/cust/train/ckpt-1\n",
            "Epoch 5 Loss 4.8718 Accuracy 0.2358\n",
            "Time taken for 1 epoch: 59.69 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 4.7836 Accuracy 0.2298\n",
            "Epoch 6 Batch 50 Loss 4.7662 Accuracy 0.2417\n",
            "Epoch 6 Batch 100 Loss 4.7386 Accuracy 0.2427\n",
            "Epoch 6 Batch 150 Loss 4.7169 Accuracy 0.2454\n",
            "Epoch 6 Batch 200 Loss 4.7001 Accuracy 0.2468\n",
            "Epoch 6 Batch 250 Loss 4.6812 Accuracy 0.2483\n",
            "Epoch 6 Loss 4.6675 Accuracy 0.2491\n",
            "Time taken for 1 epoch: 59.29 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.5803 Accuracy 0.2392\n",
            "Epoch 7 Batch 50 Loss 4.5817 Accuracy 0.2544\n",
            "Epoch 7 Batch 100 Loss 4.5600 Accuracy 0.2555\n",
            "Epoch 7 Batch 150 Loss 4.5430 Accuracy 0.2575\n",
            "Epoch 7 Batch 200 Loss 4.5273 Accuracy 0.2589\n",
            "Epoch 7 Batch 250 Loss 4.5106 Accuracy 0.2601\n",
            "Epoch 7 Loss 4.4991 Accuracy 0.2608\n",
            "Time taken for 1 epoch: 59.88 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 4.4679 Accuracy 0.2436\n",
            "Epoch 8 Batch 50 Loss 4.4291 Accuracy 0.2660\n",
            "Epoch 8 Batch 100 Loss 4.4043 Accuracy 0.2669\n",
            "Epoch 8 Batch 150 Loss 4.3845 Accuracy 0.2689\n",
            "Epoch 8 Batch 200 Loss 4.3688 Accuracy 0.2700\n",
            "Epoch 8 Batch 250 Loss 4.3519 Accuracy 0.2714\n",
            "Epoch 8 Loss 4.3421 Accuracy 0.2721\n",
            "Time taken for 1 epoch: 59.22 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 4.3435 Accuracy 0.2509\n",
            "Epoch 9 Batch 50 Loss 4.2828 Accuracy 0.2750\n",
            "Epoch 9 Batch 100 Loss 4.2546 Accuracy 0.2767\n",
            "Epoch 9 Batch 150 Loss 4.2339 Accuracy 0.2784\n",
            "Epoch 9 Batch 200 Loss 4.2185 Accuracy 0.2798\n",
            "Epoch 9 Batch 250 Loss 4.2022 Accuracy 0.2810\n",
            "Epoch 9 Loss 4.1932 Accuracy 0.2816\n",
            "Time taken for 1 epoch: 59.28 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 4.1722 Accuracy 0.2655\n",
            "Epoch 10 Batch 50 Loss 4.1156 Accuracy 0.2878\n",
            "Epoch 10 Batch 100 Loss 4.0921 Accuracy 0.2877\n",
            "Epoch 10 Batch 150 Loss 4.0711 Accuracy 0.2894\n",
            "Epoch 10 Batch 200 Loss 4.0558 Accuracy 0.2913\n",
            "Epoch 10 Batch 250 Loss 4.0411 Accuracy 0.2921\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/cust/train/ckpt-2\n",
            "Epoch 10 Loss 4.0328 Accuracy 0.2925\n",
            "Time taken for 1 epoch: 59.99 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 4.0363 Accuracy 0.2794\n",
            "Epoch 11 Batch 50 Loss 3.9648 Accuracy 0.2981\n",
            "Epoch 11 Batch 100 Loss 3.9476 Accuracy 0.2979\n",
            "Epoch 11 Batch 150 Loss 3.9286 Accuracy 0.2997\n",
            "Epoch 11 Batch 200 Loss 3.9152 Accuracy 0.3004\n",
            "Epoch 11 Batch 250 Loss 3.9044 Accuracy 0.3013\n",
            "Epoch 11 Loss 3.8955 Accuracy 0.3021\n",
            "Time taken for 1 epoch: 59.36 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.9462 Accuracy 0.2823\n",
            "Epoch 12 Batch 50 Loss 3.8410 Accuracy 0.3074\n",
            "Epoch 12 Batch 100 Loss 3.8200 Accuracy 0.3076\n",
            "Epoch 12 Batch 150 Loss 3.7949 Accuracy 0.3097\n",
            "Epoch 12 Batch 200 Loss 3.7793 Accuracy 0.3111\n",
            "Epoch 12 Batch 250 Loss 3.7679 Accuracy 0.3116\n",
            "Epoch 12 Loss 3.7591 Accuracy 0.3123\n",
            "Time taken for 1 epoch: 59.37 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.8101 Accuracy 0.2947\n",
            "Epoch 13 Batch 50 Loss 3.7080 Accuracy 0.3164\n",
            "Epoch 13 Batch 100 Loss 3.6870 Accuracy 0.3169\n",
            "Epoch 13 Batch 150 Loss 3.6656 Accuracy 0.3193\n",
            "Epoch 13 Batch 200 Loss 3.6493 Accuracy 0.3209\n",
            "Epoch 13 Batch 250 Loss 3.6394 Accuracy 0.3214\n",
            "Epoch 13 Loss 3.6346 Accuracy 0.3215\n",
            "Time taken for 1 epoch: 59.35 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 3.6755 Accuracy 0.3085\n",
            "Epoch 14 Batch 50 Loss 3.5730 Accuracy 0.3273\n",
            "Epoch 14 Batch 100 Loss 3.5638 Accuracy 0.3263\n",
            "Epoch 14 Batch 150 Loss 3.5449 Accuracy 0.3289\n",
            "Epoch 14 Batch 200 Loss 3.5340 Accuracy 0.3298\n",
            "Epoch 14 Batch 250 Loss 3.5258 Accuracy 0.3303\n",
            "Epoch 14 Loss 3.5227 Accuracy 0.3305\n",
            "Time taken for 1 epoch: 59.36 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 3.6156 Accuracy 0.3071\n",
            "Epoch 15 Batch 50 Loss 3.4592 Accuracy 0.3371\n",
            "Epoch 15 Batch 100 Loss 3.4480 Accuracy 0.3362\n",
            "Epoch 15 Batch 150 Loss 3.4259 Accuracy 0.3389\n",
            "Epoch 15 Batch 200 Loss 3.4124 Accuracy 0.3407\n",
            "Epoch 15 Batch 250 Loss 3.4024 Accuracy 0.3416\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/cust/train/ckpt-3\n",
            "Epoch 15 Loss 3.3957 Accuracy 0.3422\n",
            "Time taken for 1 epoch: 59.69 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 3.4288 Accuracy 0.3326\n",
            "Epoch 16 Batch 50 Loss 3.3126 Accuracy 0.3507\n",
            "Epoch 16 Batch 100 Loss 3.3009 Accuracy 0.3508\n",
            "Epoch 16 Batch 150 Loss 3.2840 Accuracy 0.3529\n",
            "Epoch 16 Batch 200 Loss 3.2724 Accuracy 0.3545\n",
            "Epoch 16 Batch 250 Loss 3.2595 Accuracy 0.3553\n",
            "Epoch 16 Loss 3.2526 Accuracy 0.3561\n",
            "Time taken for 1 epoch: 59.30 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 3.3222 Accuracy 0.3348\n",
            "Epoch 17 Batch 50 Loss 3.1916 Accuracy 0.3611\n",
            "Epoch 17 Batch 100 Loss 3.1689 Accuracy 0.3625\n",
            "Epoch 17 Batch 150 Loss 3.1510 Accuracy 0.3654\n",
            "Epoch 17 Batch 200 Loss 3.1382 Accuracy 0.3674\n",
            "Epoch 17 Batch 250 Loss 3.1282 Accuracy 0.3682\n",
            "Epoch 17 Loss 3.1217 Accuracy 0.3691\n",
            "Time taken for 1 epoch: 59.27 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 3.2124 Accuracy 0.3508\n",
            "Epoch 18 Batch 50 Loss 3.0838 Accuracy 0.3743\n",
            "Epoch 18 Batch 100 Loss 3.0620 Accuracy 0.3745\n",
            "Epoch 18 Batch 150 Loss 3.0415 Accuracy 0.3774\n",
            "Epoch 18 Batch 200 Loss 3.0286 Accuracy 0.3787\n",
            "Epoch 18 Batch 250 Loss 3.0176 Accuracy 0.3800\n",
            "Epoch 18 Loss 3.0123 Accuracy 0.3805\n",
            "Time taken for 1 epoch: 59.48 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 3.1182 Accuracy 0.3545\n",
            "Epoch 19 Batch 50 Loss 2.9682 Accuracy 0.3868\n",
            "Epoch 19 Batch 100 Loss 2.9442 Accuracy 0.3878\n",
            "Epoch 19 Batch 150 Loss 2.9265 Accuracy 0.3904\n",
            "Epoch 19 Batch 200 Loss 2.9146 Accuracy 0.3921\n",
            "Epoch 19 Batch 250 Loss 2.9055 Accuracy 0.3934\n",
            "Epoch 19 Loss 2.9003 Accuracy 0.3938\n",
            "Time taken for 1 epoch: 59.58 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 3.0015 Accuracy 0.3647\n",
            "Epoch 20 Batch 50 Loss 2.8584 Accuracy 0.3986\n",
            "Epoch 20 Batch 100 Loss 2.8370 Accuracy 0.4003\n",
            "Epoch 20 Batch 150 Loss 2.8264 Accuracy 0.4023\n",
            "Epoch 20 Batch 200 Loss 2.8155 Accuracy 0.4042\n",
            "Epoch 20 Batch 250 Loss 2.8081 Accuracy 0.4053\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/cust/train/ckpt-4\n",
            "Epoch 20 Loss 2.8015 Accuracy 0.4060\n",
            "Time taken for 1 epoch: 59.45 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArnRm2fruGE0"
      },
      "source": [
        "Let's test a few sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_mkMuNBh0h-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385fbd65-cee3-40f1-ec08-95854c0b2437"
      },
      "source": [
        "print('############# FIRST TEST #############')\n",
        "sentence = test_df.iloc[6]['dialogue']\n",
        "print('DIALOGUE: ' + sentence)\n",
        "print('TRUE SUMMARY: ' + test_df.iloc[6]['summary'])\n",
        "print('CUSTOM SUMMARY : ' + evaluate(sentence=sentence))\n",
        "print('\\n')\n",
        "\n",
        "print('############# SECOND TEST #############')\n",
        "sentence = test_df.iloc[10]['dialogue']\n",
        "print('DIALOGUE: ' + sentence)\n",
        "print('TRUE SUMMARY: ' + test_df.iloc[10]['summary'])\n",
        "print('CUSTOM SUMMARY : ' + evaluate(sentence=sentence))\n",
        "print('\\n')\n",
        "\n",
        "print('############# THIRD TEST #############')\n",
        "sentence = test_df.iloc[11]['dialogue']\n",
        "print('DIALOGUE: ' + sentence)\n",
        "print('TRUE SUMMARY: ' + test_df.iloc[11]['summary'])\n",
        "print('CUSTOM SUMMARY : ' + evaluate(sentence=sentence))\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############# FIRST TEST #############\n",
            "DIALOGUE: Max: Know any good sites to buy clothes from?  Payton: Sure :) <file_other> <file_other> <file_other> <file_other> <file_other> <file_other> <file_other>  Max: That's a lot of them!  Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.  Max: I'll check them out. Thanks.   Payton: No problem :)  Max: How about u?  Payton: What about me?  Max: Do u like shopping?  Payton: Yes and no.  Max: How come?  Payton: I like browsing, trying on, looking in the mirror and seeing how I look, but not always buying.  Max: Y not?  Payton: Isn't it obvious? ;)  Max: Sry ;)  Payton: If I bought everything I liked, I'd have nothing left to live on ;)  Max: Same here, but probably different category ;)  Payton: Lol  Max: So what do u usually buy?  Payton: Well, I have 2 things I must struggle to resist!  Max: Which are?  Payton: Clothes, ofc ;)  Max: Right. And the second one?  Payton: Books. I absolutely love reading!  Max: Gr8! What books do u read?  Payton: Everything I can get my hands on :)  Max: Srsly?  Payton: Yup :)\n",
            "TRUE SUMMARY: Payton provides Max with websites selling clothes. Payton likes browsing and trying on the clothes but not necessarily buying them. Payton usually buys clothes and books as he loves reading.\n",
            "CUSTOM SUMMARY : [CLS] Max is looking for a new shop, but he doesn't like it. [SEP]\n",
            "\n",
            "\n",
            "############# SECOND TEST #############\n",
            "DIALOGUE: Wanda: Let's make a party!  Gina: Why?  Wanda: beacuse. I want some fun!  Gina: ok, what do u need?  Wanda: 1st I need too make a list  Gina: noted and then?  Wanda: well, could u take yours father car and go do groceries with me?  Gina: don't know if he'll agree  Wanda: I know, but u can ask :)  Gina: I'll try but theres no promisess  Wanda: I know, u r the best!  Gina: When u wanna go  Wanda: Friday?  Gina: ok, I'll ask\n",
            "TRUE SUMMARY: Wanda wants to throw a party. She asks Gina to borrow her father's car and go do groceries together. They set the date for Friday. \n",
            "CUSTOM SUMMARY : [CLS] Emma is in a shop but she needs a surprise for him. She will bring some wine and a coke and a bottle of wine. [SEP]\n",
            "\n",
            "\n",
            "############# THIRD TEST #############\n",
            "DIALOGUE: Martin: I won two cinema tickets!  Aggie: oh cool, how come?  Martin: online. on fb, the movie mag organized it  Aggie: so what did you do  Martin: just write a short review and that's it  Aggie: well done :) so what and when. and where?  Martin: the new film with Redford  Aggie: i guess i heard sth  Martin: it's pretty cool i heard. till the end of the week  Aggie: sounds good. we'll find time XD\n",
            "TRUE SUMMARY: Martin wrote a short review and won 2 cinema tickets on FB. Martin wants Aggie to go with him this week for the new film with Redford.\n",
            "CUSTOM SUMMARY : [CLS] The team is going to be released from the end of the year. She has been busy with a new film, but he has already started a new job. [SEP]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YeZa9zPuL5-"
      },
      "source": [
        "We see highly inaccurate summaries, which mostly don't even mention people in the dialogue. There seems to be an overfitting problem here, because some of the sentences can be repeated in very different contexts, such as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHq1Yqxkuj8f",
        "outputId": "e3e52e87-8144-44ba-eeab-2b901aeb50ed"
      },
      "source": [
        "print('############# FIRST TEST #############')\n",
        "sentence = test_df.iloc[1]['dialogue']\n",
        "print('DIALOGUE: ' + sentence)\n",
        "print('TRUE SUMMARY: ' + test_df.iloc[1]['summary'])\n",
        "print('CUSTOM SUMMARY : ' + evaluate(sentence=sentence))\n",
        "print('\\n')\n",
        "\n",
        "print('############# SECOND TEST #############')\n",
        "sentence = test_df.iloc[7]['dialogue']\n",
        "print('DIALOGUE: ' + sentence)\n",
        "print('TRUE SUMMARY: ' + test_df.iloc[7]['summary'])\n",
        "print('CUSTOM SUMMARY : ' + evaluate(sentence=sentence))\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############# FIRST TEST #############\n",
            "DIALOGUE: Eric: MACHINE!  Rob: That's so gr8!  Eric: I know! And shows how Americans see Russian ;)  Rob: And it's really funny!  Eric: I know! I especially like the train part!  Rob: Hahaha! No one talks to the machine like that!  Eric: Is this his only stand-up?  Rob: Idk. I'll check.  Eric: Sure.  Rob: Turns out no! There are some of his stand-ups on youtube.  Eric: Gr8! I'll watch them now!  Rob: Me too!  Eric: MACHINE!  Rob: MACHINE!  Eric: TTYL?  Rob: Sure :)\n",
            "TRUE SUMMARY: Eric and Rob are going to watch a stand-up on youtube.\n",
            "CUSTOM SUMMARY : [CLS] There's a bomb threat at the university. He got a new worker from the choir next month. [SEP]\n",
            "\n",
            "\n",
            "############# SECOND TEST #############\n",
            "DIALOGUE: Rita: I'm so bloody tired. Falling asleep at work. :-(  Tina: I know what you mean.  Tina: I keep on nodding off at my keyboard hoping that the boss doesn't notice..  Rita: The time just keeps on dragging on and on and on....   Rita: I keep on looking at the clock and there's still 4 hours of this drudgery to go.  Tina: Times like these I really hate my work.  Rita: I'm really not cut out for this level of boredom.  Tina: Neither am I.\n",
            "TRUE SUMMARY: Rita and Tina are bored at work and have still 4 hours left.\n",
            "CUSTOM SUMMARY : [CLS] There's a bomb threat at the university. [SEP]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHMw9Ip6uwtF"
      },
      "source": [
        "A terrifying result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgLBC12hIqZd"
      },
      "source": [
        "We realize that this architecture is better suited for machine translation. We also realize why summarization is a hard problem to deal with - nowadays translators are much more successful, but we don't see any good summarizers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCxjbg112KQV"
      },
      "source": [
        "##BERT Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_xsd-yIZnzV"
      },
      "source": [
        "Now we use BERT as encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XhnhVzgcYM2",
        "outputId": "335df5f9-c86f-482f-bd77-572f4993616c"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> dialogue, tar -> summary\n",
        "  for (batch, (inp, tar)) in enumerate(train_ds):\n",
        "    train_step_bert(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager_bert.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['cond/transformer_bert_1/tf_bert_model_1/bert/pooler/dense/kernel:0', 'cond/transformer_bert_1/tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "Epoch 1 Batch 0 Loss 10.3367 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 10.2868 Accuracy 0.0286\n",
            "Epoch 1 Batch 100 Loss 10.1932 Accuracy 0.0526\n",
            "Epoch 1 Batch 150 Loss 10.0635 Accuracy 0.0607\n",
            "Epoch 1 Batch 200 Loss 9.8934 Accuracy 0.0650\n",
            "Epoch 1 Batch 250 Loss 9.6858 Accuracy 0.0676\n",
            "Epoch 1 Loss 9.4796 Accuracy 0.0690\n",
            "Time taken for 1 epoch: 92.72 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 8.0212 Accuracy 0.0773\n",
            "Epoch 2 Batch 50 Loss 7.7415 Accuracy 0.0773\n",
            "Epoch 2 Batch 100 Loss 7.4769 Accuracy 0.0772\n",
            "Epoch 2 Batch 150 Loss 7.2572 Accuracy 0.0777\n",
            "Epoch 2 Batch 200 Loss 7.0945 Accuracy 0.0848\n",
            "Epoch 2 Batch 250 Loss 6.9692 Accuracy 0.0909\n",
            "Epoch 2 Loss 6.8770 Accuracy 0.0947\n",
            "Time taken for 1 epoch: 83.82 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 6.2938 Accuracy 0.1182\n",
            "Epoch 3 Batch 50 Loss 6.1904 Accuracy 0.1402\n",
            "Epoch 3 Batch 100 Loss 6.0773 Accuracy 0.1520\n",
            "Epoch 3 Batch 150 Loss 5.9735 Accuracy 0.1606\n",
            "Epoch 3 Batch 200 Loss 5.8837 Accuracy 0.1676\n",
            "Epoch 3 Batch 250 Loss 5.8017 Accuracy 0.1737\n",
            "Epoch 3 Loss 5.7409 Accuracy 0.1778\n",
            "Time taken for 1 epoch: 84.05 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 5.3831 Accuracy 0.1955\n",
            "Epoch 4 Batch 50 Loss 5.3328 Accuracy 0.2054\n",
            "Epoch 4 Batch 100 Loss 5.2848 Accuracy 0.2067\n",
            "Epoch 4 Batch 150 Loss 5.2454 Accuracy 0.2099\n",
            "Epoch 4 Batch 200 Loss 5.2087 Accuracy 0.2133\n",
            "Epoch 4 Batch 250 Loss 5.1731 Accuracy 0.2162\n",
            "Epoch 4 Loss 5.1459 Accuracy 0.2182\n",
            "Time taken for 1 epoch: 84.12 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 4.9845 Accuracy 0.2130\n",
            "Epoch 5 Batch 50 Loss 4.9645 Accuracy 0.2320\n",
            "Epoch 5 Batch 100 Loss 4.9268 Accuracy 0.2342\n",
            "Epoch 5 Batch 150 Loss 4.8981 Accuracy 0.2367\n",
            "Epoch 5 Batch 200 Loss 4.8716 Accuracy 0.2390\n",
            "Epoch 5 Batch 250 Loss 4.8441 Accuracy 0.2416\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/bert/train/ckpt-1\n",
            "Epoch 5 Loss 4.8231 Accuracy 0.2435\n",
            "Time taken for 1 epoch: 84.59 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 4.7035 Accuracy 0.2400\n",
            "Epoch 6 Batch 50 Loss 4.6837 Accuracy 0.2551\n",
            "Epoch 6 Batch 100 Loss 4.6472 Accuracy 0.2570\n",
            "Epoch 6 Batch 150 Loss 4.6156 Accuracy 0.2605\n",
            "Epoch 6 Batch 200 Loss 4.5864 Accuracy 0.2636\n",
            "Epoch 6 Batch 250 Loss 4.5580 Accuracy 0.2667\n",
            "Epoch 6 Loss 4.5366 Accuracy 0.2690\n",
            "Time taken for 1 epoch: 84.07 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.4087 Accuracy 0.2677\n",
            "Epoch 7 Batch 50 Loss 4.3865 Accuracy 0.2846\n",
            "Epoch 7 Batch 100 Loss 4.3449 Accuracy 0.2869\n",
            "Epoch 7 Batch 150 Loss 4.3117 Accuracy 0.2909\n",
            "Epoch 7 Batch 200 Loss 4.2834 Accuracy 0.2943\n",
            "Epoch 7 Batch 250 Loss 4.2539 Accuracy 0.2975\n",
            "Epoch 7 Loss 4.2319 Accuracy 0.2992\n",
            "Time taken for 1 epoch: 84.87 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 4.1239 Accuracy 0.2939\n",
            "Epoch 8 Batch 50 Loss 4.0807 Accuracy 0.3141\n",
            "Epoch 8 Batch 100 Loss 4.0409 Accuracy 0.3162\n",
            "Epoch 8 Batch 150 Loss 4.0079 Accuracy 0.3199\n",
            "Epoch 8 Batch 200 Loss 3.9802 Accuracy 0.3232\n",
            "Epoch 8 Batch 250 Loss 3.9515 Accuracy 0.3257\n",
            "Epoch 8 Loss 3.9306 Accuracy 0.3273\n",
            "Time taken for 1 epoch: 84.68 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 3.8539 Accuracy 0.3224\n",
            "Epoch 9 Batch 50 Loss 3.7862 Accuracy 0.3400\n",
            "Epoch 9 Batch 100 Loss 3.7525 Accuracy 0.3416\n",
            "Epoch 9 Batch 150 Loss 3.7246 Accuracy 0.3447\n",
            "Epoch 9 Batch 200 Loss 3.6998 Accuracy 0.3476\n",
            "Epoch 9 Batch 250 Loss 3.6749 Accuracy 0.3495\n",
            "Epoch 9 Loss 3.6558 Accuracy 0.3510\n",
            "Time taken for 1 epoch: 84.11 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.6513 Accuracy 0.3268\n",
            "Epoch 10 Batch 50 Loss 3.5336 Accuracy 0.3595\n",
            "Epoch 10 Batch 100 Loss 3.5000 Accuracy 0.3620\n",
            "Epoch 10 Batch 150 Loss 3.4717 Accuracy 0.3655\n",
            "Epoch 10 Batch 200 Loss 3.4476 Accuracy 0.3677\n",
            "Epoch 10 Batch 250 Loss 3.4219 Accuracy 0.3701\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/bert/train/ckpt-2\n",
            "Epoch 10 Loss 3.4037 Accuracy 0.3719\n",
            "Time taken for 1 epoch: 85.47 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.4042 Accuracy 0.3530\n",
            "Epoch 11 Batch 50 Loss 3.2904 Accuracy 0.3813\n",
            "Epoch 11 Batch 100 Loss 3.2546 Accuracy 0.3843\n",
            "Epoch 11 Batch 150 Loss 3.2310 Accuracy 0.3871\n",
            "Epoch 11 Batch 200 Loss 3.2069 Accuracy 0.3897\n",
            "Epoch 11 Batch 250 Loss 3.1823 Accuracy 0.3921\n",
            "Epoch 11 Loss 3.1650 Accuracy 0.3933\n",
            "Time taken for 1 epoch: 84.69 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.1991 Accuracy 0.3837\n",
            "Epoch 12 Batch 50 Loss 3.0541 Accuracy 0.4047\n",
            "Epoch 12 Batch 100 Loss 3.0257 Accuracy 0.4065\n",
            "Epoch 12 Batch 150 Loss 3.0070 Accuracy 0.4088\n",
            "Epoch 12 Batch 200 Loss 2.9874 Accuracy 0.4104\n",
            "Epoch 12 Batch 250 Loss 2.9705 Accuracy 0.4119\n",
            "Epoch 12 Loss 2.9550 Accuracy 0.4133\n",
            "Time taken for 1 epoch: 84.69 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.0306 Accuracy 0.3844\n",
            "Epoch 13 Batch 50 Loss 2.8665 Accuracy 0.4206\n",
            "Epoch 13 Batch 100 Loss 2.8359 Accuracy 0.4227\n",
            "Epoch 13 Batch 150 Loss 2.8231 Accuracy 0.4244\n",
            "Epoch 13 Batch 200 Loss 2.8068 Accuracy 0.4262\n",
            "Epoch 13 Batch 250 Loss 2.7900 Accuracy 0.4279\n",
            "Epoch 13 Loss 2.7756 Accuracy 0.4293\n",
            "Time taken for 1 epoch: 85.07 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.8728 Accuracy 0.4114\n",
            "Epoch 14 Batch 50 Loss 2.7076 Accuracy 0.4386\n",
            "Epoch 14 Batch 100 Loss 2.6776 Accuracy 0.4406\n",
            "Epoch 14 Batch 150 Loss 2.6640 Accuracy 0.4416\n",
            "Epoch 14 Batch 200 Loss 2.6480 Accuracy 0.4430\n",
            "Epoch 14 Batch 250 Loss 2.6337 Accuracy 0.4440\n",
            "Epoch 14 Loss 2.6205 Accuracy 0.4456\n",
            "Time taken for 1 epoch: 84.88 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.7246 Accuracy 0.4114\n",
            "Epoch 15 Batch 50 Loss 2.5638 Accuracy 0.4523\n",
            "Epoch 15 Batch 100 Loss 2.5298 Accuracy 0.4554\n",
            "Epoch 15 Batch 150 Loss 2.5106 Accuracy 0.4581\n",
            "Epoch 15 Batch 200 Loss 2.4941 Accuracy 0.4605\n",
            "Epoch 15 Batch 250 Loss 2.4791 Accuracy 0.4622\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/bert/train/ckpt-3\n",
            "Epoch 15 Loss 2.4622 Accuracy 0.4644\n",
            "Time taken for 1 epoch: 85.58 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.5408 Accuracy 0.4376\n",
            "Epoch 16 Batch 50 Loss 2.3762 Accuracy 0.4760\n",
            "Epoch 16 Batch 100 Loss 2.3464 Accuracy 0.4794\n",
            "Epoch 16 Batch 150 Loss 2.3263 Accuracy 0.4823\n",
            "Epoch 16 Batch 200 Loss 2.3074 Accuracy 0.4855\n",
            "Epoch 16 Batch 250 Loss 2.2909 Accuracy 0.4879\n",
            "Epoch 16 Loss 2.2772 Accuracy 0.4897\n",
            "Time taken for 1 epoch: 84.30 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.3690 Accuracy 0.4748\n",
            "Epoch 17 Batch 50 Loss 2.1927 Accuracy 0.5014\n",
            "Epoch 17 Batch 100 Loss 2.1643 Accuracy 0.5050\n",
            "Epoch 17 Batch 150 Loss 2.1499 Accuracy 0.5071\n",
            "Epoch 17 Batch 200 Loss 2.1351 Accuracy 0.5092\n",
            "Epoch 17 Batch 250 Loss 2.1200 Accuracy 0.5113\n",
            "Epoch 17 Loss 2.1068 Accuracy 0.5130\n",
            "Time taken for 1 epoch: 83.86 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 2.1712 Accuracy 0.4880\n",
            "Epoch 18 Batch 50 Loss 2.0189 Accuracy 0.5266\n",
            "Epoch 18 Batch 100 Loss 2.0032 Accuracy 0.5295\n",
            "Epoch 18 Batch 150 Loss 1.9905 Accuracy 0.5318\n",
            "Epoch 18 Batch 200 Loss 1.9762 Accuracy 0.5338\n",
            "Epoch 18 Batch 250 Loss 1.9623 Accuracy 0.5364\n",
            "Epoch 18 Loss 1.9517 Accuracy 0.5381\n",
            "Time taken for 1 epoch: 83.78 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 2.0564 Accuracy 0.4996\n",
            "Epoch 19 Batch 50 Loss 1.8901 Accuracy 0.5482\n",
            "Epoch 19 Batch 100 Loss 1.8694 Accuracy 0.5512\n",
            "Epoch 19 Batch 150 Loss 1.8537 Accuracy 0.5545\n",
            "Epoch 19 Batch 200 Loss 1.8383 Accuracy 0.5576\n",
            "Epoch 19 Batch 250 Loss 1.8268 Accuracy 0.5594\n",
            "Epoch 19 Loss 1.8168 Accuracy 0.5609\n",
            "Time taken for 1 epoch: 83.81 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.8765 Accuracy 0.5485\n",
            "Epoch 20 Batch 50 Loss 1.7696 Accuracy 0.5696\n",
            "Epoch 20 Batch 100 Loss 1.7518 Accuracy 0.5709\n",
            "Epoch 20 Batch 150 Loss 1.7391 Accuracy 0.5723\n",
            "Epoch 20 Batch 200 Loss 1.7223 Accuracy 0.5758\n",
            "Epoch 20 Batch 250 Loss 1.7113 Accuracy 0.5787\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/bert/train/ckpt-4\n",
            "Epoch 20 Loss 1.7028 Accuracy 0.5800\n",
            "Time taken for 1 epoch: 84.20 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFTO-TChJTjP"
      },
      "source": [
        "We see slightly longer runtimes, but we can see the model accuracy is considerably higher than before, 58% vs 40%, and lower loss of 1.7 vs. 2.8. These results are impressive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co-SU59Knj6O"
      },
      "source": [
        "Let's take a look at a few generated summaries below. Here's my personal favourite summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-aFPlKJPwTH"
      },
      "source": [
        "sentence = test_df.iloc[2]['dialogue']\n",
        "true_summ = test_df.iloc[2]['summary']\n",
        "pred_summ_bert = evaluate(sentence=sentence,use_bert=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "T4z0AUikROeq",
        "outputId": "d8bb7279-79be-47b3-bbbc-ccc93dbca21c"
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Lenny: Babe, can you help me with something?  Bob: Sure, what's up?  Lenny: Which one should I pick?  Bob: Send me photos  Lenny:  <file_photo>  Lenny:  <file_photo>  Lenny:  <file_photo>  Bob: I like the first ones best  Lenny: But I already have purple trousers. Does it make sense to have two pairs?  Bob: I have four black pairs :D :D  Lenny: yeah, but shouldn't I pick a different color?  Bob: what matters is what you'll give you the most outfit options  Lenny: So I guess I'll buy the first or the third pair then  Bob: Pick the best quality then  Lenny: ur right, thx  Bob: no prob :)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xIMNHtKmRPsg",
        "outputId": "4f338e3e-8d95-456e-9d24-179dff8a8a18"
      },
      "source": [
        "true_summ"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6TsX4xMBRnP-",
        "outputId": "f94cc7bb-4479-49d6-a7c7-71c561ec6b89"
      },
      "source": [
        "pred_summ"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] Bob has a new trousers. He will pick up Bob a pink trousers. [SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RikPzstPJkN2"
      },
      "source": [
        "The result is not accurate, however it has one interesting aspect: it talks about pink trousers but \"pink\" is never mentioned in the original dialogue. The abstraction here generated new information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGKU5RcYnr5t"
      },
      "source": [
        "Let's see the summaries for other dialogues:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFWlGkxifkjI",
        "outputId": "4b5c3926-3865-4679-9bb8-7ea9309cb5dc"
      },
      "source": [
        "print('############# FIRST TEST #############')\n",
        "sentence = test_df.iloc[0]['dialogue']\n",
        "print('DIALOGUE: ' + sentence)\n",
        "print('TRUE SUMMARY: ' + test_df.iloc[0]['summary'])\n",
        "print('BERT SUMMARY : ' + evaluate(sentence=sentence,use_bert=True))\n",
        "print('\\n')\n",
        "\n",
        "print('############# SECOND TEST #############')\n",
        "sentence = test_df.iloc[1]['dialogue']\n",
        "print('DIALOGUE: ' + sentence)\n",
        "print('TRUE SUMMARY: ' + test_df.iloc[1]['summary'])\n",
        "print('BERT SUMMARY : ' + evaluate(sentence=sentence,use_bert=True))\n",
        "print('\\n')\n",
        "\n",
        "print('############# THIRD TEST #############')\n",
        "sentence = test_df.iloc[3]['dialogue']\n",
        "print('DIALOGUE: ' + sentence)\n",
        "print('TRUE SUMMARY: ' + test_df.iloc[3]['summary'])\n",
        "print('BERT SUMMARY : ' + evaluate(sentence=sentence,use_bert=True))\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############# FIRST TEST #############\n",
            "DIALOGUE: Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him 🙂 Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye\n",
            "TRUE SUMMARY: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
            "BERT SUMMARY : [CLS] Amanda is worried about Amanda's number to Amanda's advice on it. Hannah and Hannah's number is very older and he's very well. Hannah and Hannah have a number of HRr D's number of useful on a\n",
            "\n",
            "\n",
            "############# SECOND TEST #############\n",
            "DIALOGUE: Eric: MACHINE!  Rob: That's so gr8!  Eric: I know! And shows how Americans see Russian ;)  Rob: And it's really funny!  Eric: I know! I especially like the train part!  Rob: Hahaha! No one talks to the machine like that!  Eric: Is this his only stand-up?  Rob: Idk. I'll check.  Eric: Sure.  Rob: Turns out no! There are some of his stand-ups on youtube.  Eric: Gr8! I'll watch them now!  Rob: Me too!  Eric: MACHINE!  Rob: MACHINE!  Eric: TTYL?  Rob: Sure :)\n",
            "TRUE SUMMARY: Eric and Rob are going to watch a stand-up on youtube.\n",
            "BERT SUMMARY : [CLS] Rob used to enjoy his old movies but Rob is angry with Rob. Rob and Rob want to watch a tweeting. Rob is unsure about it. Rob is unsure about Rob, but Rob is very angry. [SEP]\n",
            "\n",
            "\n",
            "############# THIRD TEST #############\n",
            "DIALOGUE: Will: hey babe, what do you want for dinner tonight?  Emma:  gah, don't even worry about it tonight  Will: what do you mean? everything ok?  Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry  Will: Well what time will you be home?  Emma: soon, hopefully  Will: you sure? Maybe you want me to pick you up?  Emma: no no it's alright. I'll be home soon, i'll tell you when I get home.   Will: Alright, love you.   Emma: love you too. \n",
            "TRUE SUMMARY: Emma will be home soon and she will let Will know.\n",
            "BERT SUMMARY : [CLS] Will is hungry and will have dinner tonight. Will will pick up Emma and Will and Will will have dinner tonight. [SEP]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPHQR4tXu_Jm"
      },
      "source": [
        "We clearly see the difference compared to the previous model - BERT actually refers to the people in the dialogue, can give some context on what's going on in the dialogue (such as dinner, movies, phone number). It's flawed but we have a winner!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw_-jSH92QOR"
      },
      "source": [
        "##T5 Summarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul3INijbyIlm"
      },
      "source": [
        "While researching for my project, I came across to T5 model **[4]**, and realized it had summarization capabilities. That's why I wanted to briefly compare it to my models. It uses pytorch:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e14vwqGH1iCS"
      },
      "source": [
        "!pip install transformers==2.8.0\n",
        "!pip install torch==1.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raxsVyZ1VxE2"
      },
      "source": [
        "import torch\n",
        "import json \n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cML5Dvzd2q8d"
      },
      "source": [
        "An example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94rj_KTv3MzB"
      },
      "source": [
        "t5model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "t5tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "device = torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Loa1_4tUTNi",
        "outputId": "2e3edabd-e9b6-4344-e4a7-8fd9503b415c"
      },
      "source": [
        "text =\"\"\"\n",
        "The US has \"passed the peak\" on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month.\n",
        "The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.\n",
        "At the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors.\n",
        "\"We'll be the comeback kids, all of us,\" he said. \"We want to get our country back.\"\n",
        "The Trump administration has previously fixed May 1 as a possible date to reopen the world's largest economy, but the president said some states may be able to return to normalcy earlier than that.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
        "t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "print (\"original text preprocessed: \\n\", preprocess_text)\n",
        "\n",
        "tokenized_text = t5tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "# summmarize \n",
        "summary_ids = t5model.generate(tokenized_text,\n",
        "                                    num_beams=4,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    min_length=30,\n",
        "                                    max_length=100,\n",
        "                                    early_stopping=True)\n",
        "\n",
        "output = t5tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print (\"\\n\\nSummarized text: \\n\",output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original text preprocessed: \n",
            " The US has \"passed the peak\" on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month.The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.At the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors.\"We'll be the comeback kids, all of us,\" he said. \"We want to get our country back.\"The Trump administration has previously fixed May 1 as a possible date to reopen the world's largest economy, but the president said some states may be able to return to normalcy earlier than that.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " the us has over 637,000 confirmed Covid-19 cases and over 30,826 deaths. president Donald Trump predicts some states will reopen the country in april, he said. \"we'll be the comeback kids, all of us,\" the president says.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIdF1Xf338i0"
      },
      "source": [
        "Works good with an article. Let's see how it fares with dialogues:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R32Vos602vkL",
        "outputId": "bd3e2b45-b5eb-428f-cda6-3e235755af82"
      },
      "source": [
        "for i in range(4):\n",
        "    preprocess_text = test_df.iloc[i]['dialogue']\n",
        "    t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "    print (\"\\n\\noriginal text preprocessed: \\n\", preprocess_text)\n",
        "\n",
        "    tokenized_text = t5tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "    # summmarize \n",
        "    summary_ids = t5model.generate(tokenized_text,\n",
        "                                        num_beams=4,\n",
        "                                        no_repeat_ngram_size=2,\n",
        "                                        min_length=5,\n",
        "                                        max_length=40,\n",
        "                                        early_stopping=True)\n",
        "\n",
        "    output = t5tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print (\"\\nSummarized text: \\n\",output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "original text preprocessed: \n",
            " Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him 🙂 Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye\n",
            "\n",
            "Summarized text: \n",
            " Amanda: Lemme check Hannah: file_gif> Amanda. he called her last time we were at the park together Hannah. I don't know him well Hannah\n",
            "\n",
            "\n",
            "original text preprocessed: \n",
            " Eric: MACHINE!  Rob: That's so gr8!  Eric: I know! And shows how Americans see Russian ;)  Rob: And it's really funny!  Eric: I know! I especially like the train part!  Rob: Hahaha! No one talks to the machine like that!  Eric: Is this his only stand-up?  Rob: Idk. I'll check.  Eric: Sure.  Rob: Turns out no! There are some of his stand-ups on youtube.  Eric: Gr8! I'll watch them now!  Rob: Me too!  Eric: MACHINE!  Rob: MACHINE!  Eric: TTYL?  Rob: Sure :)\n",
            "\n",
            "Summarized text: \n",
            " there are some of his stand-ups on youtube. Eric: Idk. I'll check. Rob: Turns out no! there's a lot of\n",
            "\n",
            "\n",
            "original text preprocessed: \n",
            " Lenny: Babe, can you help me with something?  Bob: Sure, what's up?  Lenny: Which one should I pick?  Bob: Send me photos  Lenny:  <file_photo>  Lenny:  <file_photo>  Lenny:  <file_photo>  Bob: I like the first ones best  Lenny: But I already have purple trousers. Does it make sense to have two pairs?  Bob: I have four black pairs :D :D  Lenny: yeah, but shouldn't I pick a different color?  Bob: what matters is what you'll give you the most outfit options  Lenny: So I guess I'll buy the first or the third pair then  Bob: Pick the best quality then  Lenny: ur right, thx  Bob: no prob :)\n",
            "\n",
            "Summarized text: \n",
            " Bob: what matters is what you'll give you the most outfit options Lenny. he: pick the best quality then ur right, thx Bob :\n",
            "\n",
            "\n",
            "original text preprocessed: \n",
            " Will: hey babe, what do you want for dinner tonight?  Emma:  gah, don't even worry about it tonight  Will: what do you mean? everything ok?  Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry  Will: Well what time will you be home?  Emma: soon, hopefully  Will: you sure? Maybe you want me to pick you up?  Emma: no no it's alright. I'll be home soon, i'll tell you when I get home.   Will: Alright, love you.   Emma: love you too. \n",
            "\n",
            "Summarized text: \n",
            " hey babe, what do you want for dinner tonight? Emma: gah, don't worry about cooking though, I'm not hungry Will: Well what time will you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYYqtjpq4WZL"
      },
      "source": [
        "It's acting more like an extractive summarization, just isolating the parts it find the most relevant. The results are underwhelming (and slow), but the extractive nature can be very useful for customer calls, if someone is looking whether a specific topic/term was discussed. Ultimately, BERT model performs much better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mFlyrlucBNs"
      },
      "source": [
        "##Further Discussion, Further Improvements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q32WME8BgIsV"
      },
      "source": [
        "Firstly, as we see above, the BERT model was very impressive compared to our custom-built encoder. Its abstraction level is primarily relevant, and the training accuracy has grown visibly epoch-to-epoch. And T5 is better suited for article/news summarizations.\n",
        "\n",
        "And personally I learnt that there are more resources using pytorch than tensorflow for this problem (higgingface tutorials are mostly in tensorflow).\n",
        "\n",
        "\n",
        "Here is some future work that could be carried:\n",
        "1.   Combining a state-of-art model such as BERT with a vanilla decoder is a big disservice to BERT. Although we still see impressive results, we need to find a better way to incorporate BERT's power into the transformer's decoder part.\n",
        "2.   We only used a \"tiny\" BERT model. Still, there are bigger models with more hidden layers and number of heads. We can easily use pre-trained models to save computation power. Still, BERT is trained on Wikipedia data, and it might not be as powerful when applied to dialogues.\n",
        "3.   We only used accuracy as a measure during this project - **[2]** uses ROUGE metrics to measure the transformer-generated summaries' quality.\n",
        "4.   Similar to **[2]**, we could've news data to the model. However, the size of the CNN dataset they have used is significant, and we need more computational power.\n",
        "5.   Although our model was not suitable for more hyperparameter tuning, there is an opportunity for more pre-processing. Although we were very constricted due to the computationally heavy transformer model, \n",
        "6.   We could explore breaking down the dialogues into first and second-person texts while keeping the positional encodings. This would give us more context about who said what because we observed that the model could get confused.\n",
        "7.   It might also be interesting to compare BERT performance to the OpenNMT library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZiKFK8Ceo7i"
      },
      "source": [
        "##References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiQxZFm_etXF"
      },
      "source": [
        "**[1]** *Tensorflow: \"Transformer model for language understanding\"* [https://www.tensorflow.org/tutorials/text/transformer]\n",
        "\n",
        "Used to build the transformer architecture\n",
        "\n",
        "**[2]** *SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization* [https://arxiv.org/abs/1911.12237]\n",
        "\n",
        "Dialogue Dataset used in the models\n",
        "\n",
        "**[3]** *Hugging Face* [https://huggingface.co/]\n",
        "\n",
        "Used BERT tokenizer for both models, BERT encoder to replace the trainable encoder from the original architecture, and T5 summarizer to compare it to these results\n",
        "\n",
        "**[4]** *Abstractive Text Summarization using T5* [https://github.com/faiztariq/FzLabs/blob/master/abstractive-text-summarization-t5.ipynb]\n",
        "\n",
        "Checking the performance when applied to dialogues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_FPN8FbKlk9"
      },
      "source": [
        "Some other reads:\n",
        "\n",
        "https://towardsdatascience.com/practical-nlp-summarising-short-and-long-speeches-with-hugging-faces-pipeline-bc7df76bd366\n",
        "\n",
        "https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d\n",
        "\n",
        "https://medium.com/rocket-mortgage-technology-blog/conversational-summarization-with-natural-language-processing-c073a6bcaa3a\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/classify_text_with_bert\n",
        "\n",
        "https://skimai.com/fine-tuning-bert-for-sentiment-analysis/\n"
      ]
    }
  ]
}